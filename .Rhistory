printcp(treeFitted)
# Plot the decision tree
rpart.plot(treeFitted, type = 3, extra = 101)
# Create a new data frame for prediction
trainingDataSet <- data.frame(R_Head_split._Touch = 32, B_Head_split._Touch = 14, WINNER=2)
# Make prediction
prediction <- predict(treeFitted, newdata = trainingDataSet, type = "class")
# Print the prediction
print(paste("The predicted winner is: ", prediction))
table_mat <- table(data_test$survived, predict_unseen)
# Charger le package nécessaire
library(rpart)
library(rpart.plot)
# Charger les données
df <- read.csv("preprocessing_data_UFC.csv")
# Fit a decision tree using the Gini index
treeFitted <- rpart(WINNER ~ R_Head_split._Touch + B_Head_split._Touch, data = df, method = "class")
# Print the resulting decision tree
printcp(treeFitted)
# Plot the decision tree
rpart.plot(treeFitted, type = 3, extra = 101)
# Create a new data frame for prediction
trainingDataSet <- data.frame(R_Head_split._Touch = 32, B_Head_split._Touch = 14, WINNER=2)
# Make prediction
prediction <- predict(treeFitted, newdata = trainingDataSet, type = "class")
# Print the prediction
print(paste("The predicted winner is: ", prediction))
create_train_test <- function(data, size = 0.8, train = TRUE) {
n_row = nrow(data)
total_row = size * n_row
train_sample < - 1: total_row
if (train == TRUE) {
return (data[train_sample, ])
} else {
return (data[-train_sample, ])
}
}
data_train <- create_train_test(clean_titanic, 0.8, train = TRUE)
create_train_test <- function(data, size = 0.8, train = TRUE) {
n_row = nrow(data)
total_row = size * n_row
train_sample < - 1: total_row
if (train == TRUE) {
return (data[train_sample, ])
} else {
return (data[-train_sample, ])
}
}
data_train <- create_train_test(df, 0.8, train = TRUE)
create_train_test <- function(data, size = 0.8, train = TRUE) {
n_row = nrow(data)
total_row = size * n_row
train_sample = 1: total_row
if (train == TRUE) {
return (data[train_sample, ])
} else {
return (data[-train_sample, ])
}
}
data_train <- create_train_test(df, 0.8, train = TRUE)
# Charger le package nécessaire
library(rpart)
library(rpart.plot)
# Charger les données
df <- read.csv("preprocessing_data_UFC.csv")
# Fit a decision tree using the Gini index
treeFitted <- rpart(WINNER ~ R_Head_split._Touch + B_Head_split._Touch, data = df, method = "class")
# Print the resulting decision tree
printcp(treeFitted)
# Plot the decision tree
rpart.plot(treeFitted, type = 3, extra = 101)
# Create a new data frame for prediction
trainingDataSet <- data.frame(R_Head_split._Touch = 32, B_Head_split._Touch = 14, WINNER=2)
# Make prediction
prediction <- predict(treeFitted, newdata = trainingDataSet, type = "class")
# Print the prediction
print(paste("The predicted winner is: ", prediction))
View(data_train)
View(data_train)
length(data_train)
size(data_train)
dim(data_train)
create_train_test <- function(data, size = 0.8, train = TRUE) {
n_row = nrow(data)
total_row = size * n_row
train_sample = 1: total_row
if (train == TRUE) {
return (data[train_sample, ])
} else {
return (data[-train_sample, ])
}
}
data_train <- create_train_test(df, 0.8, train = TRUE)
data_train <- create_train_test(df, 0.8, train = FALSE)
# Charger le package nécessaire
library(rpart)
library(rpart.plot)
# Charger les données
df <- read.csv("preprocessing_data_UFC.csv")
# Fit a decision tree using the Gini index
treeFitted <- rpart(WINNER ~ R_Head_split._Touch + B_Head_split._Touch, data = df, method = "class")
# Print the resulting decision tree
printcp(treeFitted)
# Plot the decision tree
rpart.plot(treeFitted, type = 3, extra = 101)
# Create a new data frame for prediction
trainingDataSet <- data.frame(R_Head_split._Touch = 32, B_Head_split._Touch = 14, WINNER=2)
# Make prediction
prediction <- predict(treeFitted, newdata = trainingDataSet, type = "class")
# Print the prediction
print(paste("The predicted winner is: ", prediction))
create_train_test <- function(data, size = 0.8, train = TRUE) {
n_row = nrow(data)
total_row = size * n_row
train_sample = 1: total_row
if (train == TRUE) {
return (data[train_sample, ])
} else {
return (data[-train_sample, ])
}
}
data_train <- create_train_test(df, 0.8, train = TRUE)
data_test <- create_train_test(df, 0.8, train = FALSE)
# Charger le package nécessaire
library(rpart)
library(rpart.plot)
# Charger les données
df <- read.csv("preprocessing_data_UFC.csv")
# Fit a decision tree using the Gini index
treeFitted <- rpart(WINNER ~ R_Head_split._Touch + B_Head_split._Touch, data = df, method = "class")
# Print the resulting decision tree
printcp(treeFitted)
# Plot the decision tree
rpart.plot(treeFitted, type = 3, extra = 101)
# Create a new data frame for prediction
trainingDataSet <- data.frame(R_Head_split._Touch = 32, B_Head_split._Touch = 14, WINNER=2)
# Make prediction
prediction <- predict(treeFitted, newdata = trainingDataSet, type = "class")
# Print the prediction
print(paste("The predicted winner is: ", prediction))
dim(data_test)
dim(data_train)
1203+4809
create_train_test <- function(data, size = 0.8, train = TRUE) {
n_row = nrow(data)
total_row = size * n_row
train_sample = 1: total_row
if (train == TRUE) {
return (data[train_sample, ])
} else {
return (data[-train_sample, ])
}
}
data_train <- create_train_test(df, 0.8, train = TRUE)
data_test <- create_train_test(df, 0.8, train = FALSE)
# Charger le package nécessaire
library(rpart)
library(rpart.plot)
# Charger les données
df <- read.csv("preprocessing_data_UFC.csv")
# Fit a decision tree using the Gini index
treeFitted <- rpart(WINNER ~ R_Head_split._Touch + B_Head_split._Touch, data = data_train, method = "class")
# Plot the decision tree
rpart.plot(treeFitted, type = 3, extra = 106)
create_train_test <- function(data, size = 0.8, train = TRUE) {
n_row = nrow(data)
total_row = size * n_row
train_sample = 1: total_row
if (train == TRUE) {
return (data[train_sample, ])
} else {
return (data[-train_sample, ])
}
}
data_train <- create_train_test(df, 0.8, train = TRUE)
data_test <- create_train_test(df, 0.8, train = FALSE)
# Charger le package nécessaire
library(rpart)
library(rpart.plot)
# Charger les données
df <- read.csv("preprocessing_data_UFC.csv")
# Fit a decision tree using the Gini index
treeFitted <- rpart(WINNER ~ R_Head_split._Touch + B_Head_split._Touch, data = data_train, method = "class")
# Plot the decision tree
rpart.plot(treeFitted, type = 3, extra = 101)
# Make prediction
prediction <- predict(treeFitted, data_test, type = "class")
prediction
create_train_test <- function(data, size = 0.8, train = TRUE) {
n_row = nrow(data)
total_row = size * n_row
train_sample = 1: total_row
if (train == TRUE) {
return (data[train_sample, ])
} else {
return (data[-train_sample, ])
}
}
data_train <- create_train_test(df, 0.8, train = TRUE)
data_test <- create_train_test(df, 0.8, train = FALSE)
# Charger le package nécessaire
library(rpart)
library(rpart.plot)
# Charger les données
df <- read.csv("preprocessing_data_UFC.csv")
# Fit a decision tree using the Gini index
treeFitted <- rpart(WINNER ~ R_Head_split._Touch + B_Head_split._Touch, data = data_train, method = "class")
# Plot the decision tree
rpart.plot(treeFitted, type = 3, extra = 101)
# Create a new data frame for prediction
trainingDataSet <- data.frame(R_Head_split._Touch = 32, B_Head_split._Touch = 14, WINNER=2)
# Make prediction
prediction <- predict(treeFitted, data_test, type = "class")
# Print the prediction
print(paste("The predicted winner is: ", prediction))
table_mat <- table(data_test$WINNER, prediction)
table_mat
accuracy_Test <- sum(diag(table_mat)) / sum(table_mat)
accuracy_Test
length(which(data_test$WINNER == 2))
length(which(data_test$WINNER == 1))
length(which(data_test$WINNER == 0))
create_train_test <- function(data, size = 0.8, train = TRUE) {
n_row = nrow(data)
total_row = size * n_row
train_sample = 1: total_row
if (train == TRUE) {
return (data[train_sample, ])
} else {
return (data[-train_sample, ])
}
}
data_train <- create_train_test(df, 0.8, train = TRUE)
data_test <- create_train_test(df, 0.8, train = FALSE)
# Charger le package nécessaire
library(rpart)
library(rpart.plot)
# Charger les données
df <- read.csv("preprocessing_data_UFC.csv")
# Définir les paramètres de contrôle
control <- rpart.control(cp = 0.01, # complexité du modèle
minsplit = 20, # nombre minimum d'observations pour diviser un nœud
minbucket = 5, # nombre minimum d'observations dans chaque feuille
maxdepth = 30) # profondeur maximale de l'arbre
# Fit a decision tree using the Gini index
treeFitted <- rpart(WINNER ~ R_Head_split._Touch + B_Head_split._Touch, data = data_train, method = "class",control=control)
# Plot the decision tree
rpart.plot(treeFitted, type = 3, extra = 101)
# Create a new data frame for prediction
trainingDataSet <- data.frame(R_Head_split._Touch = 32, B_Head_split._Touch = 14, WINNER=2)
# Make prediction
prediction <- predict(treeFitted, data_test, type = "class")
# Print the prediction
print(paste("The predicted winner is: ", prediction))
table_mat <- table(data_test$WINNER, prediction)
table_mat
accuracy_Test <- sum(diag(table_mat)) / sum(table_mat)
length(which(data_test$WINNER==1))
length(which(data_train$WINNER==1))
length(which(data_train$WINNER==0))
length(which(data_train$WINNER==2))
length(which(data_train$WINNER==2))
length(which(data_test$WINNER==2))
length(which(data_test$WINNER==0))
length(which(data_test$WINNER==1))
length(which(data_train$WINNER==1))
length(which(data_train$WINNER==0))
create_train_test <- function(data, size = 0.8, train = TRUE) {
n_row = nrow(data)
total_row = size * n_row
train_sample = 1: total_row
if (train == TRUE) {
return (data[train_sample, ])
} else {
return (data[-train_sample, ])
}
}
data_train <- create_train_test(df, 0.8, train = TRUE)
data_test <- create_train_test(df, 0.8, train = FALSE)
# Charger le package nécessaire
library(rpart)
library(rpart.plot)
# Charger les données
df <- read.csv("preprocessing_data_UFC.csv")
# Définir les paramètres de contrôle
control <- rpart.control(cp = 0.01, # complexité du modèle
minsplit = 20, # nombre minimum d'observations pour diviser un nœud
minbucket = 5, # nombre minimum d'observations dans chaque feuille
maxdepth = 30) # profondeur maximale de l'arbre
# Fit a decision tree using the Gini index
treeFitted <- rpart(WINNER ~ R_Head_split._Touch + B_Head_split._Touch, data = data_train, method = "class",control=control)
# Plot the decision tree
rpart.plot(treeFitted, type = 3, extra = 101)
# Create a new data frame for prediction
trainingDataSet <- data.frame(R_Head_split._Touch = 32, B_Head_split._Touch = 14, WINNER=2)
# Make prediction
prediction <- predict(treeFitted, data_test, type = "class")
# Print the prediction
print(paste("The predicted winner is: ", prediction))
table_mat <- table(data_test$WINNER, prediction)
table_mat
accuracy_Test <- sum(diag(table_mat)) / sum(table_mat)
table_mat
length(which(data_test$WINNER=1))
length(which(data_test$WINNER==1))
length(which(data_test$WINNER==0))
data_test
View(data_test)
View(data_test)
data_test$WINNER[1]=0
# Charger le package nécessaire
library(rpart)
library(rpart.plot)
# Charger les données
df <- read.csv("preprocessing_data_UFC.csv")
# Définir les paramètres de contrôle
control <- rpart.control(cp = 0.01, # complexité du modèle
minsplit = 20, # nombre minimum d'observations pour diviser un nœud
minbucket = 5, # nombre minimum d'observations dans chaque feuille
maxdepth = 30) # profondeur maximale de l'arbre
# Fit a decision tree using the Gini index
treeFitted <- rpart(WINNER ~ R_Head_split._Touch + B_Head_split._Touch, data = data_train, method = "class",control=control)
# Plot the decision tree
rpart.plot(treeFitted, type = 3, extra = 101)
# Create a new data frame for prediction
trainingDataSet <- data.frame(R_Head_split._Touch = 32, B_Head_split._Touch = 14, WINNER=2)
# Make prediction
prediction <- predict(treeFitted, data_test, type = "class")
# Print the prediction
print(paste("The predicted winner is: ", prediction))
table_mat <- table(data_test$WINNER, prediction)
table_mat
accuracy_Test <- sum(diag(table_mat)) / sum(table_mat)
# Charger le package nécessaire
library(rpart)
library(rpart.plot)
# Charger les données
df <- read.csv("preprocessing_data_UFC.csv")
# Définir les paramètres de contrôle
control <- rpart.control(cp = 0.01, # complexité du modèle
minsplit = 4, # nombre minimum d'observations pour diviser un nœud
minbucket = 5/3, # nombre minimum d'observations dans chaque feuille
maxdepth = 3) # profondeur maximale de l'arbre
# Fit a decision tree using the Gini index
treeFitted <- rpart(WINNER ~ R_Head_split._Touch + B_Head_split._Touch, data = data_train, method = "class",control=control)
# Plot the decision tree
rpart.plot(treeFitted, type = 3, extra = 101)
# Create a new data frame for prediction
trainingDataSet <- data.frame(R_Head_split._Touch = 32, B_Head_split._Touch = 14, WINNER=2)
# Make prediction
prediction <- predict(treeFitted, data_test, type = "class")
# Print the prediction
print(paste("The predicted winner is: ", prediction))
table_mat <- table(data_test$WINNER, prediction)
table_mat
accuracy_Test <- sum(diag(table_mat)) / sum(table_mat)
# Charger le package nécessaire
library(rpart)
library(rpart.plot)
# Charger les données
df <- read.csv("preprocessing_data_UFC.csv")
# Définir les paramètres de contrôle
control <- rpart.control(cp = 0.01, # complexité du modèle
minsplit = 4, # nombre minimum d'observations pour diviser un nœud
minbucket = 5/3, # nombre minimum d'observations dans chaque feuille
maxdepth = 2) # profondeur maximale de l'arbre
# Fit a decision tree using the Gini index
treeFitted <- rpart(WINNER ~ R_Head_split._Touch + B_Head_split._Touch, data = data_train, method = "class",control=control)
# Plot the decision tree
rpart.plot(treeFitted, type = 3, extra = 101)
# Create a new data frame for prediction
trainingDataSet <- data.frame(R_Head_split._Touch = 32, B_Head_split._Touch = 14, WINNER=2)
# Make prediction
prediction <- predict(treeFitted, data_test, type = "class")
# Print the prediction
print(paste("The predicted winner is: ", prediction))
table_mat <- table(data_test$WINNER, prediction)
table_mat
accuracy_Test <- sum(diag(table_mat)) / sum(table_mat)
data_train <- create_train_test(df, 0.8, train = TRUE)
data_test <- create_train_test(df, 0.8, train = FALSE)
# Charger le package nécessaire
library(rpart)
library(rpart.plot)
# Charger les données
df <- read.csv("preprocessing_data_UFC.csv")
# Définir les paramètres de contrôle
control <- rpart.control(cp = 0.01, # complexité du modèle
minsplit = 4, # nombre minimum d'observations pour diviser un nœud
minbucket = 5/3, # nombre minimum d'observations dans chaque feuille
maxdepth = 3) # profondeur maximale de l'arbre
# Fit a decision tree using the Gini index
treeFitted <- rpart(WINNER ~ R_Head_split._Touch + B_Head_split._Touch, data = data_train, method = "class",control=control)
# Plot the decision tree
rpart.plot(treeFitted, type = 3, extra = 101)
# Create a new data frame for prediction
trainingDataSet <- data.frame(R_Head_split._Touch = 32, B_Head_split._Touch = 14, WINNER=2)
# Make prediction
prediction <- predict(treeFitted, data_test, type = "class")
# Print the prediction
print(paste("The predicted winner is: ", prediction))
table_mat <- table(data_test$WINNER, prediction)
table_mat
accuracy_Test <- sum(diag(table_mat)) / sum(table_mat)
# Charger le package nécessaire
library(rpart)
library(rpart.plot)
# Charger les données
df <- read.csv("preprocessing_data_UFC.csv")
# Définir les paramètres de contrôle
control <- rpart.control(cp = 0.01, # complexité du modèle
minsplit = 2, # nombre minimum d'observations pour diviser un nœud
minbucket = 5/3, # nombre minimum d'observations dans chaque feuille
maxdepth = 3) # profondeur maximale de l'arbre
# Fit a decision tree using the Gini index
treeFitted <- rpart(WINNER ~ R_Head_split._Touch + B_Head_split._Touch, data = data_train, method = "class",control=control)
# Plot the decision tree
rpart.plot(treeFitted, type = 3, extra = 101)
# Create a new data frame for prediction
trainingDataSet <- data.frame(R_Head_split._Touch = 32, B_Head_split._Touch = 14, WINNER=2)
# Make prediction
prediction <- predict(treeFitted, data_test, type = "class")
# Print the prediction
print(paste("The predicted winner is: ", prediction))
table_mat <- table(data_test$WINNER, prediction)
table_mat
accuracy_Test <- sum(diag(table_mat)) / sum(table_mat)
# Charger le package nécessaire
library(rpart)
library(rpart.plot)
# Charger les données
df <- read.csv("preprocessing_data_UFC.csv")
# Définir les paramètres de contrôle
control <- rpart.control(cp = 0.01, # complexité du modèle
minsplit = 20, # nombre minimum d'observations pour diviser un nœud
minbucket = 5/3, # nombre minimum d'observations dans chaque feuille
maxdepth = 3) # profondeur maximale de l'arbre
# Fit a decision tree using the Gini index
treeFitted <- rpart(WINNER ~ R_Head_split._Touch + B_Head_split._Touch, data = data_train, method = "class",control=control)
# Plot the decision tree
rpart.plot(treeFitted, type = 3, extra = 101)
# Create a new data frame for prediction
trainingDataSet <- data.frame(R_Head_split._Touch = 32, B_Head_split._Touch = 14, WINNER=2)
# Make prediction
prediction <- predict(treeFitted, data_test, type = "class")
# Print the prediction
print(paste("The predicted winner is: ", prediction))
table_mat <- table(data_test$WINNER, prediction)
table_mat
accuracy_Test <- sum(diag(table_mat)) / sum(table_mat)
# Charger les données
df <- read.csv("preprocessing_data_UFC.csv")
# Définir les paramètres de contrôle
control <- rpart.control(cp = 0.01, # complexité du modèle
minsplit = 4, # nombre minimum d'observations pour diviser un nœud
minbucket = 5/3, # nombre minimum d'observations dans chaque feuille
maxdepth = 3) # profondeur maximale de l'arbre
# Fit a decision tree using the Gini index
treeFitted <- rpart(WINNER ~ R_Head_split._Touch + B_Head_split._Touch, data = data_train, method = "class",control=control)
# Plot the decision tree
rpart.plot(treeFitted, type = 3, extra = 101)
# Create a new data frame for prediction
trainingDataSet <- data.frame(R_Head_split._Touch = 32, B_Head_split._Touch = 14, WINNER=2)
# Make prediction
prediction <- predict(treeFitted, data_test, type = "class")
# Print the prediction
print(paste("The predicted winner is: ", prediction))
table_mat <- table(data_test$WINNER, prediction)
table_mat
accuracy_Test <- sum(diag(table_mat)) / sum(table_mat)
accuracy_Test
create_train_test <- function(data, size = 0.8, train = TRUE) {
n_row = nrow(data)
total_row = size * n_row
train_sample = 1: total_row
if (train == TRUE) {
return (data[train_sample, ])
} else {
return (data[-train_sample, ])
}
}
data_train <- create_train_test(df, 0.8, train = TRUE)
data_test <- create_train_test(df, 0.8, train = FALSE)
# a voir comment faire j'ai mit cette ligne car sinon matrice rectanguaire et donc pb dans le calcul des matrice de confusion
data_test$WINNER[1]=0
# Charger le package nécessaire
library(rpart)
library(rpart.plot)
# Charger les données
df <- read.csv("preprocessing_data_UFC.csv")
# Définir les paramètres de contrôle
control <- rpart.control(cp = 0.01, # complexité du modèle
minsplit = 4, # nombre minimum d'observations pour diviser un nœud
minbucket = 5/3, # nombre minimum d'observations dans chaque feuille
maxdepth = 3) # profondeur maximale de l'arbre
# Fit a decision tree using the Gini index
treeFitted <- rpart(WINNER ~ R_Head_split._Touch + B_Head_split._Touch, data = data_train, method = "class",control=control)
# Plot the decision tree
rpart.plot(treeFitted, type = 3, extra = 101)
# Create a new data frame for prediction
trainingDataSet <- data.frame(R_Head_split._Touch = 32, B_Head_split._Touch = 14, WINNER=2)
# Make prediction
prediction <- predict(treeFitted, data_test, type = "class")
# Print the prediction
print(paste("The predicted winner is: ", prediction))
table_mat <- table(data_test$WINNER, prediction)
table_mat
accuracy_Test <- sum(diag(table_mat)) / sum(table_mat)
length(which(data_train$WINNER=0))
length(which(data_train$WINNER==0))
length(which(data_train$WINNER==2))
length(which(data_train$WINNER==1))
